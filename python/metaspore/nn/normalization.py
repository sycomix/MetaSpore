#
# Copyright 2022 DMetaSoul
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import torch
import torch.nn.functional as F

class Normalization(torch.nn.modules.batchnorm._BatchNorm):
    def _check_input_dim(self, input):
        if input.dim() not in [2, 3]:
            raise ValueError(f'expected 3D or 3D input (got {input.dim()}D input)')

    def forward(self, input):
        if not self.training:
            return F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias, False)

        self._check_input_dim(input)

        exponential_average_factor = 0.0 if self.momentum is None else self.momentum
        if self.training and self.track_running_stats:
            if self.num_batches_tracked is not None:
                self.num_batches_tracked = self.num_batches_tracked + 1
                exponential_average_factor = (
                    1.0 / float(self.num_batches_tracked)
                    if self.momentum is None
                    else self.momentum
                )
        if self.training:
            bn_training = True
        else:
            bn_training = (self.running_mean is None) and (self.running_var is None)

        batch_mean = input.mean(dim=0)
        batch_var = ((input - self.running_mean) * (input - self.running_mean)).mean(dim=0)
        output = (input - self.running_mean) / (self.running_var + self.eps).sqrt()
        if self.training:
            with torch.no_grad():
                self.running_mean[...] = batch_mean
                self.running_var[...] = batch_var
        return output * self.weight + self.bias
